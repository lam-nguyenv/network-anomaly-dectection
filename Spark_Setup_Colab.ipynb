{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Setup_Colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamvng/network-anomaly-dectection/blob/master/Spark_Setup_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52nORSJ81rUD",
        "colab_type": "text"
      },
      "source": [
        "# How to install Pyspark on Google Colab\n",
        "*Ba-Tuan THAI, Van-Lam NGUYEN, Anh-Duc PHAM - PFIEV K60*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNtnUjFe3GrY",
        "colab_type": "text"
      },
      "source": [
        "## **1. Install openjdk 8, download and configure Pyspark**\n",
        "Firstly, activate [Google Colab](https://colab.research.google.com/) by your Google account. You will be prompted to open a new Jupyter Notebook file, all of which will be stored on your Drive. Then download and install `pyspark` on your Google Colab directory.\n",
        "\n",
        "*Note*: `findspark` is a Python package to automatically locate `pyspark` installation files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4GIsY78a5aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHiDllLlg6N4",
        "colab_type": "text"
      },
      "source": [
        "Set up environmental variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyoxuUMXcJe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8566ZZbocK3S",
        "colab_type": "code",
        "outputId": "190d75a2-5c9a-4612-b6fa-5c375df643a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1hQidzrcjb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX0BhIO8g1T9",
        "colab_type": "code",
        "outputId": "398a8969-520d-460e-de5a-32b7dfee22a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link = \"https://drive.google.com/open?id=1wOl76pErCKeFwFnPRk_u-56STpqva2JQ\"\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1wOl76pErCKeFwFnPRk_u-56STpqva2JQ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZ4frfRhJW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('kddcup.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGQrpMe9hUvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsnxsTphuSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataWithoutHeader = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.csv\")\n",
        "data = dataWithoutHeader.toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
        "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
        "\"label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnJsrZk4iMgc",
        "colab_type": "code",
        "outputId": "ed7e343d-dd71-4fe3-a91a-d52f5fb66cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+------+\n",
            "|           label| count|\n",
            "+----------------+------+\n",
            "|          smurf.|280790|\n",
            "|        neptune.|107201|\n",
            "|         normal.| 97278|\n",
            "|           back.|  2203|\n",
            "|          satan.|  1589|\n",
            "|        ipsweep.|  1247|\n",
            "|      portsweep.|  1040|\n",
            "|    warezclient.|  1020|\n",
            "|       teardrop.|   979|\n",
            "|            pod.|   264|\n",
            "|           nmap.|   231|\n",
            "|   guess_passwd.|    53|\n",
            "|buffer_overflow.|    30|\n",
            "|           land.|    21|\n",
            "|    warezmaster.|    20|\n",
            "|           imap.|    12|\n",
            "|        rootkit.|    10|\n",
            "|     loadmodule.|     9|\n",
            "|      ftp_write.|     8|\n",
            "|       multihop.|     7|\n",
            "|            phf.|     4|\n",
            "|           perl.|     3|\n",
            "|            spy.|     2|\n",
            "+----------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq-t-8_5iTda",
        "colab_type": "code",
        "outputId": "6c4c3aca-f783-4cb8-ca78-ccfa31a6a782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.clustering import KMeansModel\n",
        "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\")\n",
        "assembler = VectorAssembler(inputCols=numericOnly.columns[0:-1], outputCol=\"featureVector\")\n",
        "kmeans = KMeans().setSeed(10).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
        "pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "pipelineModel = pipeline.fit(numericOnly)\n",
        "kmeansModel = pipelineModel.stages[-1]\n",
        "centers = kmeansModel.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster Centers: \n",
            "[4.79793956e+01 1.62207883e+03 8.68534183e+02 4.45326100e-05\n",
            " 6.43293794e-03 1.41694668e-05 3.45168212e-02 1.51815716e-04\n",
            " 1.48247035e-01 1.02121372e-02 1.11331525e-04 3.64357718e-05\n",
            " 1.13517671e-02 1.08295211e-03 1.09307315e-04 1.00805635e-03\n",
            " 0.00000000e+00 0.00000000e+00 1.38658354e-03 3.32286248e+02\n",
            " 2.92907143e+02 1.76685418e-01 1.76607809e-01 5.74330999e-02\n",
            " 5.77183920e-02 7.91548844e-01 2.09816404e-02 2.89968625e-02\n",
            " 2.32470732e+02 1.88666046e+02 7.53781203e-01 3.09056111e-02\n",
            " 6.01935529e-01 6.68351484e-03 1.76753957e-01 1.76441622e-01\n",
            " 5.81176268e-02 5.74111170e-02]\n",
            "[2.0000000e+00 6.9337564e+08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.7000000e+01\n",
            " 3.0000000e+00 7.9000000e-01 6.7000000e-01 2.1000000e-01 3.3000000e-01\n",
            " 5.0000000e-02 3.9000000e-01 0.0000000e+00 2.5500000e+02 3.0000000e+00\n",
            " 1.0000000e-02 9.0000000e-02 2.2000000e-01 0.0000000e+00 1.8000000e-01\n",
            " 6.7000000e-01 5.0000000e-02 3.3000000e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbqN5E0ikuI",
        "colab_type": "code",
        "outputId": "61f327a4-9287-401e-e511-1230b5d65bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "withCluster = pipelineModel.transform(numericOnly)\n",
        "withCluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------------+------+\n",
            "|cluster|           label| count|\n",
            "+-------+----------------+------+\n",
            "|      0|          smurf.|280790|\n",
            "|      0|        neptune.|107201|\n",
            "|      0|         normal.| 97278|\n",
            "|      0|           back.|  2203|\n",
            "|      0|          satan.|  1589|\n",
            "|      0|        ipsweep.|  1247|\n",
            "|      0|      portsweep.|  1039|\n",
            "|      0|    warezclient.|  1020|\n",
            "|      0|       teardrop.|   979|\n",
            "|      0|            pod.|   264|\n",
            "|      0|           nmap.|   231|\n",
            "|      0|   guess_passwd.|    53|\n",
            "|      0|buffer_overflow.|    30|\n",
            "|      0|           land.|    21|\n",
            "|      0|    warezmaster.|    20|\n",
            "|      0|           imap.|    12|\n",
            "|      0|        rootkit.|    10|\n",
            "|      0|     loadmodule.|     9|\n",
            "|      0|      ftp_write.|     8|\n",
            "|      0|       multihop.|     7|\n",
            "|      0|            phf.|     4|\n",
            "|      0|           perl.|     3|\n",
            "|      0|            spy.|     2|\n",
            "|      1|      portsweep.|     1|\n",
            "+-------+----------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0biNxLciqhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from random import randrange\n",
        "def clusteringScore0(data, k):\n",
        "    assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"featureVector\")\n",
        "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "    kmeansModel = pipeline.fit(data).stages[-1]\n",
        "    print(k, kmeansModel.computeCost(assembler.transform(data)) / data.count())\n",
        "def clusteringScore1(data, k):\n",
        "    assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"featureVector\", maxIter=40, tol=1.0e-05)\n",
        "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "    kmeansModel = pipeline.fit(data).stages[-1]\n",
        "    print(k, kmeansModel.computeCost(assembler.transform(data)) / data.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B82u3GONitjH",
        "colab_type": "code",
        "outputId": "a73bc6f6-cccd-4b7b-ff91-fd9af67b4574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for k in range(20, 120, 20):\n",
        "    clusteringScore0(numericOnly, k)\n",
        "    clusteringScore1(numericOnly, k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 70090529.18766987\n",
            "20 70090529.18766987\n",
            "40 34134989.30719846\n",
            "40 34134989.30719846\n",
            "60 32241636.217287987\n",
            "60 32241469.66946356\n",
            "80 31426292.4634663\n",
            "80 31426292.4634663\n",
            "100 29985935.77407783\n",
            "100 26300705.9737499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzEjv99Ijl1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "def clusteringScore2(data, k):\n",
        "  assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "  kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "  pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
        "  pipelineModel = pipeline.fit(data)\n",
        "  kmeansModel = pipelineModel.stages[-1]\n",
        "  print(k, kmeansModel.computeCost(pipelineModel.transform(data)) / data.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsyr0BLNkMyP",
        "colab_type": "code",
        "outputId": "98025ed5-4212-4f5f-de82-edcd40ae0afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for k in range(60, 300, 30):\n",
        "  print(k, clusteringScore2(numericOnly, k))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 1.1868969161190808\n",
            "60 None\n",
            "90 0.716842007702905\n",
            "90 None\n",
            "120 0.49734934242366263\n",
            "120 None\n",
            "150 0.3759175320833906\n",
            "150 None\n",
            "180 0.3172011058253099\n",
            "180 None\n",
            "210 0.273636909577068\n",
            "210 None\n",
            "240 0.23059890300407695\n",
            "240 None\n",
            "270 0.20361107540315934\n",
            "270 None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MaGZ36GoZB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "import math\n",
        "def oneHotPipeline(inputCol):\n",
        "    indexer = StringIndexer(inputCol=inputCol, outputCol=(inputCol + \"_indexed\"))\n",
        "    encoder = OneHotEncoder(inputCol=inputCol + \"_indexed\", outputCol=inputCol + \"_vec\")\n",
        "    pipeline = Pipeline(stages=[indexer, encoder])\n",
        "    return (pipeline, inputCol + \"_vec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZt7mjdOqCTv",
        "colab_type": "code",
        "outputId": "bdcbdcdb-5cc2-43b8-ef17-52304e837271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def clusteringScore3(data, k):\n",
        "    (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
        "    (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
        "    (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
        "    assembleCols = list(set(data.columns) - set([\"label\", \"protocol_type\", \"service\", \"flag\"])) + list([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
        "    assembler = VectorAssembler(inputCols=assembleCols, outputCol=\"featureVector\")\n",
        "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "    pipeline = Pipeline(stages=[protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
        "    pipelineModel = pipeline.fit(data)\n",
        "    kmeansModel = pipelineModel.stages[-1]\n",
        "    print(k, kmeansModel.computeCost(pipelineModel.transform(data)) / data.count())\n",
        "\n",
        "for k in range(60, 270, 30):\n",
        "    print(clusteringScore3(data, k))\n",
        "    numericOnly.unpersist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 34.32659398810193\n",
            "None\n",
            "90 10.102528717704125\n",
            "None\n",
            "120 2.956985135173616\n",
            "None\n",
            "150 2.1403963857555346\n",
            "None\n",
            "180 1.5556825369949714\n",
            "None\n",
            "210 1.2852764808975832\n",
            "None\n",
            "240 0.9366045728591509\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpDgnedqmSvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def entropy(counts):\n",
        "  values = counts.filter(lambda x: x > 0)\n",
        "  n = values.map(lambda x: float(x)).reduce(lambda x, y: x + y)\n",
        "  return values.map(lambda x: x / n).map(lambda p: -p * math.log10(p)).reduce(lambda x, y: x + y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B65IGK6XssG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fitPipeline4(data, k):\n",
        "  (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
        "  (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
        "  (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
        "\n",
        "  assembleCols = list(set(data.columns) - set([\"label\", \"protocol_type\", \"service\", \"flag\"])) + list([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
        "  assembler = VectorAssembler(inputCols=assembleCols, outputCol=\"featureVector\")\n",
        "  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "  kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "  pipeline = Pipeline(stages=[protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
        "  return pipeline.fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spNYqUIsstwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clusteringScore4(data, k):\n",
        "  pipelineModel = fitPipeline4(data, k)\n",
        "  # Predict cluster for each datum\n",
        "  clusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\")\n",
        "  weightedClusterEntropy = clusterLabel.groupByKey(lambda cluster, x: cluster).mapGroups(lambda x, cluster: )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oH2O6qcxqtE",
        "colab_type": "code",
        "outputId": "764cd409-7749-4bd8-c49c-c8762fd61cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "!./spark-2.4.4-bin-hadoop2.7/bin/spark-shell "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/11/30 14:46:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "19/11/30 14:46:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Spark context Web UI available at http://3274256e6a43:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1575125212999).\n",
            "Spark session available as 'spark'.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
            "      /_/\n",
            "         \n",
            "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_222)\n",
            "Type in expressions to have them evaluated.\n",
            "Type :help for more information.\n",
            "\n",
            "scala> val 5\n",
            "     | thai bat aun\n",
            "<console>:2: error: '=' expected but ';' found.\n",
            "thai bat aun\n",
            "^\n",
            "\n",
            "scala> "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}