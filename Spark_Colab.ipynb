{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamvng/network-anomaly-dectection/blob/master/Spark_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1w4UT7QvC9R",
        "colab_type": "text"
      },
      "source": [
        "# Network Anomalies Detection with K-Means on Pyspark\n",
        "*Ba-Tuan THAI, Van-Lam NGUYEN, Anh-Duc PHAM - PFIEV K60*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrSx7hgvOWQ",
        "colab_type": "text"
      },
      "source": [
        "## **1. Install openjdk 8, download and configure Pyspark**\n",
        "Firstly, activate [Google Colab](https://colab.research.google.com/) by your Google account. You will be prompted to open a new Jupyter Notebook file, all of which will be stored on your Drive. Then download and install `pyspark` on your Google Colab directory.\n",
        "\n",
        "*Note*: `findspark` is a Python package to automatically locate `pyspark` installation files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4GIsY78a5aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxVglWaCvau_",
        "colab_type": "text"
      },
      "source": [
        "Set up environmental variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyoxuUMXcJe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8566ZZbocK3S",
        "colab_type": "code",
        "outputId": "c4783467-e343-4ce9-80b7-8714fa03317e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1hQidzrcjb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX0BhIO8g1T9",
        "colab_type": "code",
        "outputId": "f4f1a677-bfdb-49d0-bcf6-ce593eef4f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link = \"https://drive.google.com/open?id=1wOl76pErCKeFwFnPRk_u-56STpqva2JQ\"\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1wOl76pErCKeFwFnPRk_u-56STpqva2JQ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEZ4frfRhJW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('kddcup.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGQrpMe9hUvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsnxsTphuSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataWithoutHeader = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.csv\")\n",
        "data = dataWithoutHeader.toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
        "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
        "\"label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnJsrZk4iMgc",
        "colab_type": "code",
        "outputId": "2ce35806-90f5-49c9-d4fc-4910d5b468c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+------+\n",
            "|           label| count|\n",
            "+----------------+------+\n",
            "|          smurf.|280790|\n",
            "|        neptune.|107201|\n",
            "|         normal.| 97278|\n",
            "|           back.|  2203|\n",
            "|          satan.|  1589|\n",
            "|        ipsweep.|  1247|\n",
            "|      portsweep.|  1040|\n",
            "|    warezclient.|  1020|\n",
            "|       teardrop.|   979|\n",
            "|            pod.|   264|\n",
            "|           nmap.|   231|\n",
            "|   guess_passwd.|    53|\n",
            "|buffer_overflow.|    30|\n",
            "|           land.|    21|\n",
            "|    warezmaster.|    20|\n",
            "|           imap.|    12|\n",
            "|        rootkit.|    10|\n",
            "|     loadmodule.|     9|\n",
            "|      ftp_write.|     8|\n",
            "|       multihop.|     7|\n",
            "|            phf.|     4|\n",
            "|           perl.|     3|\n",
            "|            spy.|     2|\n",
            "+----------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq-t-8_5iTda",
        "colab_type": "code",
        "outputId": "b063fb5d-951d-4286-c87b-d5f79b589258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.clustering import KMeansModel\n",
        "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\")\n",
        "assembler = VectorAssembler(inputCols=numericOnly.columns[0:-1], outputCol=\"featureVector\")\n",
        "kmeans = KMeans().setSeed(10).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
        "pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "pipelineModel = pipeline.fit(numericOnly)\n",
        "kmeansModel = pipelineModel.stages[-1]\n",
        "centers = kmeansModel.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster Centers: \n",
            "[4.79793956e+01 1.62207883e+03 8.68534183e+02 4.45326100e-05\n",
            " 6.43293794e-03 1.41694668e-05 3.45168212e-02 1.51815716e-04\n",
            " 1.48247035e-01 1.02121372e-02 1.11331525e-04 3.64357718e-05\n",
            " 1.13517671e-02 1.08295211e-03 1.09307315e-04 1.00805635e-03\n",
            " 0.00000000e+00 0.00000000e+00 1.38658354e-03 3.32286248e+02\n",
            " 2.92907143e+02 1.76685418e-01 1.76607809e-01 5.74330999e-02\n",
            " 5.77183920e-02 7.91548844e-01 2.09816404e-02 2.89968625e-02\n",
            " 2.32470732e+02 1.88666046e+02 7.53781203e-01 3.09056111e-02\n",
            " 6.01935529e-01 6.68351484e-03 1.76753957e-01 1.76441622e-01\n",
            " 5.81176268e-02 5.74111170e-02]\n",
            "[2.0000000e+00 6.9337564e+08 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.7000000e+01\n",
            " 3.0000000e+00 7.9000000e-01 6.7000000e-01 2.1000000e-01 3.3000000e-01\n",
            " 5.0000000e-02 3.9000000e-01 0.0000000e+00 2.5500000e+02 3.0000000e+00\n",
            " 1.0000000e-02 9.0000000e-02 2.2000000e-01 0.0000000e+00 1.8000000e-01\n",
            " 6.7000000e-01 5.0000000e-02 3.3000000e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbqN5E0ikuI",
        "colab_type": "code",
        "outputId": "d0f6c5ca-8a79-4387-e614-c1966015d76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "withCluster = pipelineModel.transform(numericOnly)\n",
        "withCluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------------+------+\n",
            "|cluster|           label| count|\n",
            "+-------+----------------+------+\n",
            "|      0|          smurf.|280790|\n",
            "|      0|        neptune.|107201|\n",
            "|      0|         normal.| 97278|\n",
            "|      0|           back.|  2203|\n",
            "|      0|          satan.|  1589|\n",
            "|      0|        ipsweep.|  1247|\n",
            "|      0|      portsweep.|  1039|\n",
            "|      0|    warezclient.|  1020|\n",
            "|      0|       teardrop.|   979|\n",
            "|      0|            pod.|   264|\n",
            "|      0|           nmap.|   231|\n",
            "|      0|   guess_passwd.|    53|\n",
            "|      0|buffer_overflow.|    30|\n",
            "|      0|           land.|    21|\n",
            "|      0|    warezmaster.|    20|\n",
            "|      0|           imap.|    12|\n",
            "|      0|        rootkit.|    10|\n",
            "|      0|     loadmodule.|     9|\n",
            "|      0|      ftp_write.|     8|\n",
            "|      0|       multihop.|     7|\n",
            "|      0|            phf.|     4|\n",
            "|      0|           perl.|     3|\n",
            "|      0|            spy.|     2|\n",
            "|      1|      portsweep.|     1|\n",
            "+-------+----------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0biNxLciqhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from random import randrange\n",
        "def clusteringScore0(data, k):\n",
        "    assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"featureVector\")\n",
        "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "    kmeansModel = pipeline.fit(data).stages[-1]\n",
        "    print(k, kmeansModel.computeCost(assembler.transform(data)) / data.count())\n",
        "def clusteringScore1(data, k):\n",
        "    assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"featureVector\", maxIter=40, tol=1.0e-05)\n",
        "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
        "    kmeansModel = pipeline.fit(data).stages[-1]\n",
        "    print(k, kmeansModel.computeCost(assembler.transform(data)) / data.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B82u3GONitjH",
        "colab_type": "code",
        "outputId": "a73bc6f6-cccd-4b7b-ff91-fd9af67b4574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for k in range(20, 120, 20):\n",
        "    clusteringScore0(numericOnly, k)\n",
        "    clusteringScore1(numericOnly, k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 70090529.18766987\n",
            "20 70090529.18766987\n",
            "40 34134989.30719846\n",
            "40 34134989.30719846\n",
            "60 32241636.217287987\n",
            "60 32241469.66946356\n",
            "80 31426292.4634663\n",
            "80 31426292.4634663\n",
            "100 29985935.77407783\n",
            "100 26300705.9737499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzEjv99Ijl1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "def clusteringScore2(data, k):\n",
        "  assembler = VectorAssembler(inputCols=data.columns[0:-1], outputCol=\"featureVector\")\n",
        "  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "  kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "  pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
        "  pipelineModel = pipeline.fit(data)\n",
        "  kmeansModel = pipelineModel.stages[-1]\n",
        "  print(k, kmeansModel.computeCost(pipelineModel.transform(data)) / data.count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsyr0BLNkMyP",
        "colab_type": "code",
        "outputId": "98025ed5-4212-4f5f-de82-edcd40ae0afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for k in range(60, 300, 30):\n",
        "  print(k, clusteringScore2(numericOnly, k))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 1.1868969161190808\n",
            "60 None\n",
            "90 0.716842007702905\n",
            "90 None\n",
            "120 0.49734934242366263\n",
            "120 None\n",
            "150 0.3759175320833906\n",
            "150 None\n",
            "180 0.3172011058253099\n",
            "180 None\n",
            "210 0.273636909577068\n",
            "210 None\n",
            "240 0.23059890300407695\n",
            "240 None\n",
            "270 0.20361107540315934\n",
            "270 None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MaGZ36GoZB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "import math\n",
        "def oneHotPipeline(inputCol):\n",
        "    indexer = StringIndexer(inputCol=inputCol, outputCol=(inputCol + \"_indexed\"))\n",
        "    encoder = OneHotEncoder(inputCol=inputCol + \"_indexed\", outputCol=inputCol + \"_vec\")\n",
        "    pipeline = Pipeline(stages=[indexer, encoder])\n",
        "    return (pipeline, inputCol + \"_vec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZt7mjdOqCTv",
        "colab_type": "code",
        "outputId": "bdcbdcdb-5cc2-43b8-ef17-52304e837271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def clusteringScore3(data, k):\n",
        "    (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
        "    (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
        "    (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
        "    assembleCols = list(set(data.columns) - set([\"label\", \"protocol_type\", \"service\", \"flag\"])) + list([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
        "    assembler = VectorAssembler(inputCols=assembleCols, outputCol=\"featureVector\")\n",
        "    scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "    kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "    pipeline = Pipeline(stages=[protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
        "    pipelineModel = pipeline.fit(data)\n",
        "    kmeansModel = pipelineModel.stages[-1]\n",
        "    print(k, kmeansModel.computeCost(pipelineModel.transform(data)) / data.count())\n",
        "\n",
        "for k in range(60, 270, 30):\n",
        "    print(clusteringScore3(data, k))\n",
        "    numericOnly.unpersist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 34.32659398810193\n",
            "None\n",
            "90 10.102528717704125\n",
            "None\n",
            "120 2.956985135173616\n",
            "None\n",
            "150 2.1403963857555346\n",
            "None\n",
            "180 1.5556825369949714\n",
            "None\n",
            "210 1.2852764808975832\n",
            "None\n",
            "240 0.9366045728591509\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpDgnedqmSvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def entropy(counts):\n",
        "    values = filter(lambda x: x > 0, counts)\n",
        "    n = float(sum(values))\n",
        "    sum_entropy = 0.0\n",
        "    for v in counts:\n",
        "      sum_entropy += -(v/n) * math.log((v/n))\n",
        "    # e = map(lambda v: -(v/n) * math.log((v/n)), values)\n",
        "    # print(sum(e))\n",
        "    return sum_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD8F6kbO1n-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import groupby\n",
        "def list_group(group_list):\n",
        "  groups = []\n",
        "  for key, group in groupby(group_list):\n",
        "    groups.append(len(list(group)))\n",
        "  return groups\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B65IGK6XssG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fitPipeline4(data, k):\n",
        "  (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline(\"protocol_type\")\n",
        "  (serviceEncoder, serviceVecCol) = oneHotPipeline(\"service\")\n",
        "  (flagEncoder, flagVecCol) = oneHotPipeline(\"flag\")\n",
        "  assembleCols = list(set(data.columns) - set([\"label\", \"protocol_type\", \"service\", \"flag\"])) + list([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
        "  assembler = VectorAssembler(inputCols=assembleCols, outputCol=\"featureVector\")\n",
        "  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
        "  kmeans = KMeans(seed=1, k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-05)\n",
        "  pipeline = Pipeline(stages=[protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
        "  return pipeline.fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYiHXi-_xKFO",
        "colab_type": "text"
      },
      "source": [
        "**GroupByKey, MapGroups chỉ hỗ trợ trong Spark với Scala, Java. Tuy nhiên trong document của spark có hướng dẫn là python có những hàm mạnh mẽ tương đương, cụ thể ở đây chúng ta sẽ dùng mapGroup, map, sum của itertools**  \n",
        "Hàm dưới đây có chức năng tính entropy của 1 tập dữ liệu, công thức tính sẽ là: \n",
        "Mỗi cluster được phân cụm sẽ gồm nhiều label trùng nhau, tách tưng cluster ra riêng\n",
        "\n",
        "\n",
        "1.   Tính size của từng cluster, \n",
        "2.   Group từng cluster lại thành các labels, với số lần xuất hiện trong cluster\n",
        "3.   `entropy[i] = -(frequence/sum)*log((frequence/sum))`\n",
        "4.   `Score_of_cluster[i] = size[i] * entropy[i] `\n",
        "5.   `Sum(Score_of_cluster[i])`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spNYqUIsstwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clusteringScore4(data, k):\n",
        "  pipelineModel = fitPipeline4(data, k)\n",
        "  # Predict cluster for each datum\n",
        "  clusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\")\n",
        "  labelsInCluster = clusterLabel.rdd.groupByKey().values()\n",
        "  labelCounts = labelsInCluster.map(lambda labels: list_group(labels)).collect()\n",
        "  sum_of_labelcount = 0.0\n",
        "  for t in labelCounts:\n",
        "    sum_of_labelcount += entropy(t)*sum(t)\n",
        "  return sum_of_labelcount / data.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43E6RUKZ2MbT",
        "colab_type": "code",
        "outputId": "8fe68a06-9db6-4a04-c697-ade1143a8748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for k in range(60, 300, 30):\n",
        "  print(clusteringScore4(data, k))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.324306956113726\n",
            "1.1064688883555034\n",
            "0.8790612095658834\n",
            "0.3616672324031876\n",
            "0.28924629592417994\n",
            "0.10476604691418735\n",
            "0.2517526550133862\n",
            "0.22942454180841804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ux6DeEhzzeT",
        "colab_type": "text"
      },
      "source": [
        "*Nhìn thấy rằng cluster 210 có điểm đánh giá thấp nhất, ta sẽ thực hiện phân tích bất thường với cụm này*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[Thật sự vẫn chưa hiểu nó tính score kiểu đó có tác dụng gì :))]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Bắt đầu Anomoly Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO0pcVMkzora",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipelineModel = fitPipeline4(data, 180)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgXfY9Hi1y4C",
        "colab_type": "code",
        "outputId": "c5c0f1dd-366d-4b50-ffe8-23139d13e248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "countByClusterLabel = pipelineModel.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n",
        "countByClusterLabel.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------------+------+\n",
            "|cluster|           label| count|\n",
            "+-------+----------------+------+\n",
            "|      0|         normal.|     9|\n",
            "|      0|          smurf.|280773|\n",
            "|      1|        neptune.|   101|\n",
            "|      1|      portsweep.|     1|\n",
            "|      2|           imap.|     7|\n",
            "|      2|        neptune.|   105|\n",
            "|      3|        neptune.| 36557|\n",
            "|      3|      portsweep.|    13|\n",
            "|      4|        neptune.|   101|\n",
            "|      4|      portsweep.|     4|\n",
            "|      5|        neptune.|    89|\n",
            "|      5|          satan.|     1|\n",
            "|      6|        ipsweep.|     1|\n",
            "|      6|        neptune.|   102|\n",
            "|      6|         normal.|     1|\n",
            "|      6|      portsweep.|     1|\n",
            "|      7|        neptune.|    25|\n",
            "|      8|buffer_overflow.|     6|\n",
            "|      8|      ftp_write.|     4|\n",
            "|      8|     loadmodule.|     1|\n",
            "+-------+----------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crN9djV74Peo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.linalg import Vector\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "kMeansModel = pipelineModel.stages[-1]\n",
        "centroids = kMeansModel.clusterCenters()\n",
        "clustered = pipelineModel.transform(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdnnJyYGxwxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroids[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWLr3hRCxhTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "need_order = clustered.select(\"cluster\", \"scaledFeatureVector\").rdd.map(lambda cluster: (Vectors.squared_distance(centroids[cluster[0]], cluster[1]))).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTTZwFCk2Bwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "need_order.sort(reverse=True)\n",
        "threshold = need_order[100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM3YehzV5twe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "originalCols = data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_GsYjW6PDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distance_cluster(centroids, vec_tor):\n",
        "  return Vectors.squared_distance(centroids, vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DMfKvpdxc10",
        "colab_type": "code",
        "outputId": "26bcb61e-e787-4fbf-ba1d-7db98f07bac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "clustered.filter(lambda row_in: distance_cluster(centroids[row_in[\"cluster\"]], row_in[\"scaledFeatureVector\"]) > threshold)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-41a637e3f851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclustered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow_in\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistance_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scaledFeatureVector\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: condition should be string or Column"
          ]
        }
      ]
    }
  ]
}